{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 2.0,
  "eval_steps": 500,
  "global_step": 900,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.011111111111111112,
      "grad_norm": 2.1057939529418945,
      "learning_rate": 0.00019911111111111111,
      "loss": 9.2581,
      "step": 5
    },
    {
      "epoch": 0.022222222222222223,
      "grad_norm": 39.238075256347656,
      "learning_rate": 0.00019800000000000002,
      "loss": 8.7243,
      "step": 10
    },
    {
      "epoch": 0.03333333333333333,
      "grad_norm": 121.95398712158203,
      "learning_rate": 0.0001968888888888889,
      "loss": 7.5214,
      "step": 15
    },
    {
      "epoch": 0.044444444444444446,
      "grad_norm": 10.359376907348633,
      "learning_rate": 0.0001957777777777778,
      "loss": 1.7064,
      "step": 20
    },
    {
      "epoch": 0.05555555555555555,
      "grad_norm": 61.395538330078125,
      "learning_rate": 0.0001946666666666667,
      "loss": 0.728,
      "step": 25
    },
    {
      "epoch": 0.06666666666666667,
      "grad_norm": 3.3489837646484375,
      "learning_rate": 0.00019355555555555557,
      "loss": 0.6549,
      "step": 30
    },
    {
      "epoch": 0.07777777777777778,
      "grad_norm": 2.307413339614868,
      "learning_rate": 0.00019244444444444444,
      "loss": 0.4489,
      "step": 35
    },
    {
      "epoch": 0.08888888888888889,
      "grad_norm": 0.7944432497024536,
      "learning_rate": 0.00019133333333333334,
      "loss": 0.4201,
      "step": 40
    },
    {
      "epoch": 0.1,
      "grad_norm": 0.5852090120315552,
      "learning_rate": 0.00019022222222222224,
      "loss": 0.3308,
      "step": 45
    },
    {
      "epoch": 0.1111111111111111,
      "grad_norm": 0.40772950649261475,
      "learning_rate": 0.00018911111111111112,
      "loss": 0.2863,
      "step": 50
    },
    {
      "epoch": 0.12222222222222222,
      "grad_norm": 1.2920374870300293,
      "learning_rate": 0.000188,
      "loss": 0.2771,
      "step": 55
    },
    {
      "epoch": 0.13333333333333333,
      "grad_norm": 0.5872239470481873,
      "learning_rate": 0.0001868888888888889,
      "loss": 0.2949,
      "step": 60
    },
    {
      "epoch": 0.14444444444444443,
      "grad_norm": 0.5821953415870667,
      "learning_rate": 0.0001857777777777778,
      "loss": 0.2414,
      "step": 65
    },
    {
      "epoch": 0.15555555555555556,
      "grad_norm": 0.9082880616188049,
      "learning_rate": 0.00018466666666666666,
      "loss": 0.2118,
      "step": 70
    },
    {
      "epoch": 0.16666666666666666,
      "grad_norm": 1.09840989112854,
      "learning_rate": 0.00018355555555555557,
      "loss": 0.2796,
      "step": 75
    },
    {
      "epoch": 0.17777777777777778,
      "grad_norm": 0.5194319486618042,
      "learning_rate": 0.00018244444444444447,
      "loss": 0.2483,
      "step": 80
    },
    {
      "epoch": 0.18888888888888888,
      "grad_norm": 0.48596271872520447,
      "learning_rate": 0.00018133333333333334,
      "loss": 0.2569,
      "step": 85
    },
    {
      "epoch": 0.2,
      "grad_norm": 0.42780640721321106,
      "learning_rate": 0.00018022222222222221,
      "loss": 0.2323,
      "step": 90
    },
    {
      "epoch": 0.2111111111111111,
      "grad_norm": 1.4466545581817627,
      "learning_rate": 0.00017911111111111112,
      "loss": 0.2601,
      "step": 95
    },
    {
      "epoch": 0.2222222222222222,
      "grad_norm": 0.6203381419181824,
      "learning_rate": 0.00017800000000000002,
      "loss": 0.2286,
      "step": 100
    },
    {
      "epoch": 0.23333333333333334,
      "grad_norm": 0.5583365559577942,
      "learning_rate": 0.0001768888888888889,
      "loss": 0.2288,
      "step": 105
    },
    {
      "epoch": 0.24444444444444444,
      "grad_norm": 0.6078658103942871,
      "learning_rate": 0.0001757777777777778,
      "loss": 0.2235,
      "step": 110
    },
    {
      "epoch": 0.25555555555555554,
      "grad_norm": 0.4208137094974518,
      "learning_rate": 0.00017466666666666667,
      "loss": 0.1914,
      "step": 115
    },
    {
      "epoch": 0.26666666666666666,
      "grad_norm": 0.5562767386436462,
      "learning_rate": 0.00017355555555555557,
      "loss": 0.2005,
      "step": 120
    },
    {
      "epoch": 0.2777777777777778,
      "grad_norm": 0.5547081232070923,
      "learning_rate": 0.00017244444444444444,
      "loss": 0.1979,
      "step": 125
    },
    {
      "epoch": 0.28888888888888886,
      "grad_norm": 0.5317286849021912,
      "learning_rate": 0.00017133333333333334,
      "loss": 0.1852,
      "step": 130
    },
    {
      "epoch": 0.3,
      "grad_norm": 0.5551788210868835,
      "learning_rate": 0.00017022222222222224,
      "loss": 0.2079,
      "step": 135
    },
    {
      "epoch": 0.3111111111111111,
      "grad_norm": 0.5516059398651123,
      "learning_rate": 0.00016911111111111112,
      "loss": 0.2116,
      "step": 140
    },
    {
      "epoch": 0.32222222222222224,
      "grad_norm": 0.56204754114151,
      "learning_rate": 0.000168,
      "loss": 0.1838,
      "step": 145
    },
    {
      "epoch": 0.3333333333333333,
      "grad_norm": 0.7104960083961487,
      "learning_rate": 0.0001668888888888889,
      "loss": 0.2359,
      "step": 150
    },
    {
      "epoch": 0.34444444444444444,
      "grad_norm": 0.5541868805885315,
      "learning_rate": 0.0001657777777777778,
      "loss": 0.2036,
      "step": 155
    },
    {
      "epoch": 0.35555555555555557,
      "grad_norm": 0.7651982307434082,
      "learning_rate": 0.00016466666666666667,
      "loss": 0.1916,
      "step": 160
    },
    {
      "epoch": 0.36666666666666664,
      "grad_norm": 0.4121015965938568,
      "learning_rate": 0.00016355555555555557,
      "loss": 0.1983,
      "step": 165
    },
    {
      "epoch": 0.37777777777777777,
      "grad_norm": 0.7105529308319092,
      "learning_rate": 0.00016244444444444444,
      "loss": 0.2244,
      "step": 170
    },
    {
      "epoch": 0.3888888888888889,
      "grad_norm": 0.48522520065307617,
      "learning_rate": 0.00016133333333333334,
      "loss": 0.2078,
      "step": 175
    },
    {
      "epoch": 0.4,
      "grad_norm": 0.5263397097587585,
      "learning_rate": 0.00016022222222222222,
      "loss": 0.2479,
      "step": 180
    },
    {
      "epoch": 0.4111111111111111,
      "grad_norm": 0.4443073570728302,
      "learning_rate": 0.00015911111111111112,
      "loss": 0.2195,
      "step": 185
    },
    {
      "epoch": 0.4222222222222222,
      "grad_norm": 0.9421856999397278,
      "learning_rate": 0.00015800000000000002,
      "loss": 0.1938,
      "step": 190
    },
    {
      "epoch": 0.43333333333333335,
      "grad_norm": 0.975707471370697,
      "learning_rate": 0.00015688888888888892,
      "loss": 0.233,
      "step": 195
    },
    {
      "epoch": 0.4444444444444444,
      "grad_norm": 0.48060333728790283,
      "learning_rate": 0.00015577777777777777,
      "loss": 0.1985,
      "step": 200
    },
    {
      "epoch": 0.45555555555555555,
      "grad_norm": 0.5299403667449951,
      "learning_rate": 0.00015466666666666667,
      "loss": 0.1993,
      "step": 205
    },
    {
      "epoch": 0.4666666666666667,
      "grad_norm": 0.5357173681259155,
      "learning_rate": 0.00015355555555555557,
      "loss": 0.2024,
      "step": 210
    },
    {
      "epoch": 0.4777777777777778,
      "grad_norm": 0.7542352080345154,
      "learning_rate": 0.00015244444444444447,
      "loss": 0.2013,
      "step": 215
    },
    {
      "epoch": 0.4888888888888889,
      "grad_norm": 0.7851024866104126,
      "learning_rate": 0.00015133333333333334,
      "loss": 0.2185,
      "step": 220
    },
    {
      "epoch": 0.5,
      "grad_norm": 0.4281010329723358,
      "learning_rate": 0.00015022222222222222,
      "loss": 0.1661,
      "step": 225
    },
    {
      "epoch": 0.5111111111111111,
      "grad_norm": 0.633613646030426,
      "learning_rate": 0.00014911111111111112,
      "loss": 0.1923,
      "step": 230
    },
    {
      "epoch": 0.5222222222222223,
      "grad_norm": 1.0158532857894897,
      "learning_rate": 0.000148,
      "loss": 0.1929,
      "step": 235
    },
    {
      "epoch": 0.5333333333333333,
      "grad_norm": 0.5630307793617249,
      "learning_rate": 0.0001468888888888889,
      "loss": 0.1749,
      "step": 240
    },
    {
      "epoch": 0.5444444444444444,
      "grad_norm": 0.5418225526809692,
      "learning_rate": 0.0001457777777777778,
      "loss": 0.2188,
      "step": 245
    },
    {
      "epoch": 0.5555555555555556,
      "grad_norm": 0.6214108467102051,
      "learning_rate": 0.0001446666666666667,
      "loss": 0.2175,
      "step": 250
    },
    {
      "epoch": 0.5666666666666667,
      "grad_norm": 0.6888212561607361,
      "learning_rate": 0.00014355555555555554,
      "loss": 0.2324,
      "step": 255
    },
    {
      "epoch": 0.5777777777777777,
      "grad_norm": 0.5525200366973877,
      "learning_rate": 0.00014244444444444444,
      "loss": 0.2181,
      "step": 260
    },
    {
      "epoch": 0.5888888888888889,
      "grad_norm": 0.6261931657791138,
      "learning_rate": 0.00014133333333333334,
      "loss": 0.191,
      "step": 265
    },
    {
      "epoch": 0.6,
      "grad_norm": 0.6424556970596313,
      "learning_rate": 0.00014022222222222225,
      "loss": 0.1943,
      "step": 270
    },
    {
      "epoch": 0.6111111111111112,
      "grad_norm": 0.9228836894035339,
      "learning_rate": 0.00013911111111111112,
      "loss": 0.242,
      "step": 275
    },
    {
      "epoch": 0.6222222222222222,
      "grad_norm": 0.4899066686630249,
      "learning_rate": 0.000138,
      "loss": 0.2159,
      "step": 280
    },
    {
      "epoch": 0.6333333333333333,
      "grad_norm": 0.6209553480148315,
      "learning_rate": 0.0001368888888888889,
      "loss": 0.2061,
      "step": 285
    },
    {
      "epoch": 0.6444444444444445,
      "grad_norm": 0.48287317156791687,
      "learning_rate": 0.00013577777777777777,
      "loss": 0.1878,
      "step": 290
    },
    {
      "epoch": 0.6555555555555556,
      "grad_norm": 0.5762816071510315,
      "learning_rate": 0.00013466666666666667,
      "loss": 0.2159,
      "step": 295
    },
    {
      "epoch": 0.6666666666666666,
      "grad_norm": 0.7060329914093018,
      "learning_rate": 0.00013355555555555557,
      "loss": 0.1963,
      "step": 300
    },
    {
      "epoch": 0.6777777777777778,
      "grad_norm": 0.5039269924163818,
      "learning_rate": 0.00013244444444444447,
      "loss": 0.1934,
      "step": 305
    },
    {
      "epoch": 0.6888888888888889,
      "grad_norm": 0.58073890209198,
      "learning_rate": 0.00013133333333333332,
      "loss": 0.2248,
      "step": 310
    },
    {
      "epoch": 0.7,
      "grad_norm": 0.6932163238525391,
      "learning_rate": 0.00013022222222222222,
      "loss": 0.1934,
      "step": 315
    },
    {
      "epoch": 0.7111111111111111,
      "grad_norm": 0.6120277047157288,
      "learning_rate": 0.00012911111111111112,
      "loss": 0.1545,
      "step": 320
    },
    {
      "epoch": 0.7222222222222222,
      "grad_norm": 0.5350877046585083,
      "learning_rate": 0.00012800000000000002,
      "loss": 0.1787,
      "step": 325
    },
    {
      "epoch": 0.7333333333333333,
      "grad_norm": 0.6189847588539124,
      "learning_rate": 0.0001268888888888889,
      "loss": 0.2091,
      "step": 330
    },
    {
      "epoch": 0.7444444444444445,
      "grad_norm": 0.7926344275474548,
      "learning_rate": 0.0001257777777777778,
      "loss": 0.2002,
      "step": 335
    },
    {
      "epoch": 0.7555555555555555,
      "grad_norm": 0.4579724073410034,
      "learning_rate": 0.00012466666666666667,
      "loss": 0.1867,
      "step": 340
    },
    {
      "epoch": 0.7666666666666667,
      "grad_norm": 0.9155359268188477,
      "learning_rate": 0.00012355555555555557,
      "loss": 0.1934,
      "step": 345
    },
    {
      "epoch": 0.7777777777777778,
      "grad_norm": 0.5608800649642944,
      "learning_rate": 0.00012244444444444445,
      "loss": 0.2067,
      "step": 350
    },
    {
      "epoch": 0.7888888888888889,
      "grad_norm": 0.5234827399253845,
      "learning_rate": 0.00012133333333333335,
      "loss": 0.1805,
      "step": 355
    },
    {
      "epoch": 0.8,
      "grad_norm": 0.4332217872142792,
      "learning_rate": 0.00012022222222222223,
      "loss": 0.1631,
      "step": 360
    },
    {
      "epoch": 0.8111111111111111,
      "grad_norm": 0.5168927311897278,
      "learning_rate": 0.00011911111111111111,
      "loss": 0.1895,
      "step": 365
    },
    {
      "epoch": 0.8222222222222222,
      "grad_norm": 0.46280208230018616,
      "learning_rate": 0.000118,
      "loss": 0.1673,
      "step": 370
    },
    {
      "epoch": 0.8333333333333334,
      "grad_norm": 0.5875239968299866,
      "learning_rate": 0.0001168888888888889,
      "loss": 0.1804,
      "step": 375
    },
    {
      "epoch": 0.8444444444444444,
      "grad_norm": 0.502194344997406,
      "learning_rate": 0.00011577777777777778,
      "loss": 0.1667,
      "step": 380
    },
    {
      "epoch": 0.8555555555555555,
      "grad_norm": 0.6969529390335083,
      "learning_rate": 0.00011466666666666667,
      "loss": 0.2035,
      "step": 385
    },
    {
      "epoch": 0.8666666666666667,
      "grad_norm": 0.5342328548431396,
      "learning_rate": 0.00011355555555555557,
      "loss": 0.1651,
      "step": 390
    },
    {
      "epoch": 0.8777777777777778,
      "grad_norm": 0.9851936101913452,
      "learning_rate": 0.00011244444444444445,
      "loss": 0.232,
      "step": 395
    },
    {
      "epoch": 0.8888888888888888,
      "grad_norm": 0.5667763948440552,
      "learning_rate": 0.00011133333333333333,
      "loss": 0.1842,
      "step": 400
    },
    {
      "epoch": 0.9,
      "grad_norm": 0.6381964087486267,
      "learning_rate": 0.00011022222222222222,
      "loss": 0.221,
      "step": 405
    },
    {
      "epoch": 0.9111111111111111,
      "grad_norm": 0.7211475968360901,
      "learning_rate": 0.00010911111111111112,
      "loss": 0.2211,
      "step": 410
    },
    {
      "epoch": 0.9222222222222223,
      "grad_norm": 0.5267907381057739,
      "learning_rate": 0.00010800000000000001,
      "loss": 0.1913,
      "step": 415
    },
    {
      "epoch": 0.9333333333333333,
      "grad_norm": 0.5003678202629089,
      "learning_rate": 0.00010688888888888891,
      "loss": 0.1897,
      "step": 420
    },
    {
      "epoch": 0.9444444444444444,
      "grad_norm": 0.6773400902748108,
      "learning_rate": 0.00010577777777777777,
      "loss": 0.1578,
      "step": 425
    },
    {
      "epoch": 0.9555555555555556,
      "grad_norm": 0.4976273477077484,
      "learning_rate": 0.00010466666666666667,
      "loss": 0.1808,
      "step": 430
    },
    {
      "epoch": 0.9666666666666667,
      "grad_norm": 0.666415810585022,
      "learning_rate": 0.00010355555555555556,
      "loss": 0.1994,
      "step": 435
    },
    {
      "epoch": 0.9777777777777777,
      "grad_norm": 0.46247631311416626,
      "learning_rate": 0.00010244444444444446,
      "loss": 0.196,
      "step": 440
    },
    {
      "epoch": 0.9888888888888889,
      "grad_norm": 0.6951578259468079,
      "learning_rate": 0.00010133333333333335,
      "loss": 0.1881,
      "step": 445
    },
    {
      "epoch": 1.0,
      "grad_norm": 0.9198401570320129,
      "learning_rate": 0.00010022222222222222,
      "loss": 0.2635,
      "step": 450
    },
    {
      "epoch": 1.011111111111111,
      "grad_norm": 0.5602381825447083,
      "learning_rate": 9.911111111111112e-05,
      "loss": 0.1839,
      "step": 455
    },
    {
      "epoch": 1.0222222222222221,
      "grad_norm": 0.6506819725036621,
      "learning_rate": 9.8e-05,
      "loss": 0.1573,
      "step": 460
    },
    {
      "epoch": 1.0333333333333334,
      "grad_norm": 0.6262508034706116,
      "learning_rate": 9.68888888888889e-05,
      "loss": 0.1828,
      "step": 465
    },
    {
      "epoch": 1.0444444444444445,
      "grad_norm": 0.6582897901535034,
      "learning_rate": 9.577777777777777e-05,
      "loss": 0.191,
      "step": 470
    },
    {
      "epoch": 1.0555555555555556,
      "grad_norm": 0.5843132138252258,
      "learning_rate": 9.466666666666667e-05,
      "loss": 0.1536,
      "step": 475
    },
    {
      "epoch": 1.0666666666666667,
      "grad_norm": 0.4290715157985687,
      "learning_rate": 9.355555555555556e-05,
      "loss": 0.1643,
      "step": 480
    },
    {
      "epoch": 1.0777777777777777,
      "grad_norm": 0.7252027988433838,
      "learning_rate": 9.244444444444445e-05,
      "loss": 0.1995,
      "step": 485
    },
    {
      "epoch": 1.0888888888888888,
      "grad_norm": 0.6064447164535522,
      "learning_rate": 9.133333333333334e-05,
      "loss": 0.1543,
      "step": 490
    },
    {
      "epoch": 1.1,
      "grad_norm": 0.5256757140159607,
      "learning_rate": 9.022222222222224e-05,
      "loss": 0.1447,
      "step": 495
    },
    {
      "epoch": 1.1111111111111112,
      "grad_norm": 0.6449658870697021,
      "learning_rate": 8.911111111111111e-05,
      "loss": 0.2007,
      "step": 500
    },
    {
      "epoch": 1.1222222222222222,
      "grad_norm": 0.5926575660705566,
      "learning_rate": 8.800000000000001e-05,
      "loss": 0.1976,
      "step": 505
    },
    {
      "epoch": 1.1333333333333333,
      "grad_norm": 0.8351055383682251,
      "learning_rate": 8.68888888888889e-05,
      "loss": 0.1657,
      "step": 510
    },
    {
      "epoch": 1.1444444444444444,
      "grad_norm": 0.6242631077766418,
      "learning_rate": 8.577777777777777e-05,
      "loss": 0.1776,
      "step": 515
    },
    {
      "epoch": 1.1555555555555554,
      "grad_norm": 0.5561999082565308,
      "learning_rate": 8.466666666666667e-05,
      "loss": 0.1896,
      "step": 520
    },
    {
      "epoch": 1.1666666666666667,
      "grad_norm": 0.6623243689537048,
      "learning_rate": 8.355555555555556e-05,
      "loss": 0.2102,
      "step": 525
    },
    {
      "epoch": 1.1777777777777778,
      "grad_norm": 0.5963670015335083,
      "learning_rate": 8.244444444444445e-05,
      "loss": 0.1856,
      "step": 530
    },
    {
      "epoch": 1.1888888888888889,
      "grad_norm": 0.6136841177940369,
      "learning_rate": 8.133333333333334e-05,
      "loss": 0.1473,
      "step": 535
    },
    {
      "epoch": 1.2,
      "grad_norm": 0.6371288895606995,
      "learning_rate": 8.022222222222222e-05,
      "loss": 0.1762,
      "step": 540
    },
    {
      "epoch": 1.211111111111111,
      "grad_norm": 0.5587814450263977,
      "learning_rate": 7.911111111111111e-05,
      "loss": 0.2132,
      "step": 545
    },
    {
      "epoch": 1.2222222222222223,
      "grad_norm": 0.5802278518676758,
      "learning_rate": 7.800000000000001e-05,
      "loss": 0.1918,
      "step": 550
    },
    {
      "epoch": 1.2333333333333334,
      "grad_norm": 0.468967467546463,
      "learning_rate": 7.688888888888889e-05,
      "loss": 0.1642,
      "step": 555
    },
    {
      "epoch": 1.2444444444444445,
      "grad_norm": 0.7123115658760071,
      "learning_rate": 7.577777777777779e-05,
      "loss": 0.1801,
      "step": 560
    },
    {
      "epoch": 1.2555555555555555,
      "grad_norm": 0.5565107464790344,
      "learning_rate": 7.466666666666667e-05,
      "loss": 0.132,
      "step": 565
    },
    {
      "epoch": 1.2666666666666666,
      "grad_norm": 0.5606032609939575,
      "learning_rate": 7.355555555555556e-05,
      "loss": 0.1553,
      "step": 570
    },
    {
      "epoch": 1.2777777777777777,
      "grad_norm": 0.5351148247718811,
      "learning_rate": 7.244444444444445e-05,
      "loss": 0.186,
      "step": 575
    },
    {
      "epoch": 1.2888888888888888,
      "grad_norm": 0.6131339073181152,
      "learning_rate": 7.133333333333334e-05,
      "loss": 0.1817,
      "step": 580
    },
    {
      "epoch": 1.3,
      "grad_norm": 0.697635293006897,
      "learning_rate": 7.022222222222222e-05,
      "loss": 0.1789,
      "step": 585
    },
    {
      "epoch": 1.3111111111111111,
      "grad_norm": 0.673740029335022,
      "learning_rate": 6.911111111111111e-05,
      "loss": 0.17,
      "step": 590
    },
    {
      "epoch": 1.3222222222222222,
      "grad_norm": 0.5854550004005432,
      "learning_rate": 6.800000000000001e-05,
      "loss": 0.2013,
      "step": 595
    },
    {
      "epoch": 1.3333333333333333,
      "grad_norm": 0.6349194645881653,
      "learning_rate": 6.688888888888889e-05,
      "loss": 0.1813,
      "step": 600
    },
    {
      "epoch": 1.3444444444444446,
      "grad_norm": 0.5376546382904053,
      "learning_rate": 6.577777777777779e-05,
      "loss": 0.1575,
      "step": 605
    },
    {
      "epoch": 1.3555555555555556,
      "grad_norm": 0.49932727217674255,
      "learning_rate": 6.466666666666666e-05,
      "loss": 0.1705,
      "step": 610
    },
    {
      "epoch": 1.3666666666666667,
      "grad_norm": 0.7470110058784485,
      "learning_rate": 6.355555555555556e-05,
      "loss": 0.1898,
      "step": 615
    },
    {
      "epoch": 1.3777777777777778,
      "grad_norm": 0.6137931942939758,
      "learning_rate": 6.244444444444445e-05,
      "loss": 0.189,
      "step": 620
    },
    {
      "epoch": 1.3888888888888888,
      "grad_norm": 0.5010108947753906,
      "learning_rate": 6.133333333333334e-05,
      "loss": 0.1655,
      "step": 625
    },
    {
      "epoch": 1.4,
      "grad_norm": 0.781073808670044,
      "learning_rate": 6.0222222222222225e-05,
      "loss": 0.1899,
      "step": 630
    },
    {
      "epoch": 1.411111111111111,
      "grad_norm": 0.5555018782615662,
      "learning_rate": 5.911111111111112e-05,
      "loss": 0.1683,
      "step": 635
    },
    {
      "epoch": 1.4222222222222223,
      "grad_norm": 0.6744385361671448,
      "learning_rate": 5.8e-05,
      "loss": 0.1717,
      "step": 640
    },
    {
      "epoch": 1.4333333333333333,
      "grad_norm": 0.6605554819107056,
      "learning_rate": 5.6888888888888895e-05,
      "loss": 0.1369,
      "step": 645
    },
    {
      "epoch": 1.4444444444444444,
      "grad_norm": 0.6731258034706116,
      "learning_rate": 5.577777777777778e-05,
      "loss": 0.2063,
      "step": 650
    },
    {
      "epoch": 1.4555555555555555,
      "grad_norm": 0.5752713084220886,
      "learning_rate": 5.466666666666666e-05,
      "loss": 0.1411,
      "step": 655
    },
    {
      "epoch": 1.4666666666666668,
      "grad_norm": 0.5298593044281006,
      "learning_rate": 5.355555555555556e-05,
      "loss": 0.1339,
      "step": 660
    },
    {
      "epoch": 1.4777777777777779,
      "grad_norm": 0.5828344225883484,
      "learning_rate": 5.244444444444445e-05,
      "loss": 0.1738,
      "step": 665
    },
    {
      "epoch": 1.488888888888889,
      "grad_norm": 0.4941810369491577,
      "learning_rate": 5.133333333333333e-05,
      "loss": 0.1278,
      "step": 670
    },
    {
      "epoch": 1.5,
      "grad_norm": 0.611216127872467,
      "learning_rate": 5.0222222222222226e-05,
      "loss": 0.1942,
      "step": 675
    },
    {
      "epoch": 1.511111111111111,
      "grad_norm": 0.7250369191169739,
      "learning_rate": 4.9111111111111114e-05,
      "loss": 0.1628,
      "step": 680
    },
    {
      "epoch": 1.5222222222222221,
      "grad_norm": 0.5974965691566467,
      "learning_rate": 4.8e-05,
      "loss": 0.1704,
      "step": 685
    },
    {
      "epoch": 1.5333333333333332,
      "grad_norm": 0.6847324371337891,
      "learning_rate": 4.6888888888888895e-05,
      "loss": 0.2014,
      "step": 690
    },
    {
      "epoch": 1.5444444444444443,
      "grad_norm": 0.76138836145401,
      "learning_rate": 4.577777777777778e-05,
      "loss": 0.1822,
      "step": 695
    },
    {
      "epoch": 1.5555555555555556,
      "grad_norm": 0.531669020652771,
      "learning_rate": 4.466666666666667e-05,
      "loss": 0.1786,
      "step": 700
    },
    {
      "epoch": 1.5666666666666667,
      "grad_norm": 0.6289424300193787,
      "learning_rate": 4.355555555555556e-05,
      "loss": 0.1965,
      "step": 705
    },
    {
      "epoch": 1.5777777777777777,
      "grad_norm": 0.804415225982666,
      "learning_rate": 4.2444444444444445e-05,
      "loss": 0.1859,
      "step": 710
    },
    {
      "epoch": 1.588888888888889,
      "grad_norm": 0.6165109276771545,
      "learning_rate": 4.133333333333333e-05,
      "loss": 0.2019,
      "step": 715
    },
    {
      "epoch": 1.6,
      "grad_norm": 0.5734906196594238,
      "learning_rate": 4.022222222222222e-05,
      "loss": 0.1515,
      "step": 720
    },
    {
      "epoch": 1.6111111111111112,
      "grad_norm": 0.5045421719551086,
      "learning_rate": 3.9111111111111115e-05,
      "loss": 0.1511,
      "step": 725
    },
    {
      "epoch": 1.6222222222222222,
      "grad_norm": 0.6324246525764465,
      "learning_rate": 3.8e-05,
      "loss": 0.1572,
      "step": 730
    },
    {
      "epoch": 1.6333333333333333,
      "grad_norm": 0.5667126774787903,
      "learning_rate": 3.688888888888889e-05,
      "loss": 0.1564,
      "step": 735
    },
    {
      "epoch": 1.6444444444444444,
      "grad_norm": 0.7157060503959656,
      "learning_rate": 3.577777777777778e-05,
      "loss": 0.1865,
      "step": 740
    },
    {
      "epoch": 1.6555555555555554,
      "grad_norm": 0.806545615196228,
      "learning_rate": 3.466666666666667e-05,
      "loss": 0.1644,
      "step": 745
    },
    {
      "epoch": 1.6666666666666665,
      "grad_norm": 0.6581947207450867,
      "learning_rate": 3.355555555555556e-05,
      "loss": 0.2243,
      "step": 750
    },
    {
      "epoch": 1.6777777777777778,
      "grad_norm": 0.5514264106750488,
      "learning_rate": 3.2444444444444446e-05,
      "loss": 0.1435,
      "step": 755
    },
    {
      "epoch": 1.6888888888888889,
      "grad_norm": 0.6331616640090942,
      "learning_rate": 3.1333333333333334e-05,
      "loss": 0.1799,
      "step": 760
    },
    {
      "epoch": 1.7,
      "grad_norm": 0.6311461329460144,
      "learning_rate": 3.0222222222222225e-05,
      "loss": 0.1668,
      "step": 765
    },
    {
      "epoch": 1.7111111111111112,
      "grad_norm": 0.659976601600647,
      "learning_rate": 2.9111111111111112e-05,
      "loss": 0.1694,
      "step": 770
    },
    {
      "epoch": 1.7222222222222223,
      "grad_norm": 0.672672688961029,
      "learning_rate": 2.8000000000000003e-05,
      "loss": 0.1647,
      "step": 775
    },
    {
      "epoch": 1.7333333333333334,
      "grad_norm": 0.6978477239608765,
      "learning_rate": 2.688888888888889e-05,
      "loss": 0.1953,
      "step": 780
    },
    {
      "epoch": 1.7444444444444445,
      "grad_norm": 0.7219391465187073,
      "learning_rate": 2.5777777777777778e-05,
      "loss": 0.1635,
      "step": 785
    },
    {
      "epoch": 1.7555555555555555,
      "grad_norm": 0.5923084020614624,
      "learning_rate": 2.466666666666667e-05,
      "loss": 0.1664,
      "step": 790
    },
    {
      "epoch": 1.7666666666666666,
      "grad_norm": 0.7072826027870178,
      "learning_rate": 2.3555555555555556e-05,
      "loss": 0.1943,
      "step": 795
    },
    {
      "epoch": 1.7777777777777777,
      "grad_norm": 0.5889315009117126,
      "learning_rate": 2.2444444444444447e-05,
      "loss": 0.1551,
      "step": 800
    },
    {
      "epoch": 1.7888888888888888,
      "grad_norm": 0.6155459880828857,
      "learning_rate": 2.1333333333333335e-05,
      "loss": 0.1623,
      "step": 805
    },
    {
      "epoch": 1.8,
      "grad_norm": 0.6087190508842468,
      "learning_rate": 2.0222222222222222e-05,
      "loss": 0.1698,
      "step": 810
    },
    {
      "epoch": 1.8111111111111111,
      "grad_norm": 0.6475704908370972,
      "learning_rate": 1.9111111111111113e-05,
      "loss": 0.1829,
      "step": 815
    },
    {
      "epoch": 1.8222222222222222,
      "grad_norm": 0.47794899344444275,
      "learning_rate": 1.8e-05,
      "loss": 0.2112,
      "step": 820
    },
    {
      "epoch": 1.8333333333333335,
      "grad_norm": 0.6428822875022888,
      "learning_rate": 1.688888888888889e-05,
      "loss": 0.2153,
      "step": 825
    },
    {
      "epoch": 1.8444444444444446,
      "grad_norm": 0.6167281866073608,
      "learning_rate": 1.577777777777778e-05,
      "loss": 0.2284,
      "step": 830
    },
    {
      "epoch": 1.8555555555555556,
      "grad_norm": 0.6622503399848938,
      "learning_rate": 1.4666666666666668e-05,
      "loss": 0.1352,
      "step": 835
    },
    {
      "epoch": 1.8666666666666667,
      "grad_norm": 0.6931849718093872,
      "learning_rate": 1.3555555555555557e-05,
      "loss": 0.1765,
      "step": 840
    },
    {
      "epoch": 1.8777777777777778,
      "grad_norm": 0.4487077593803406,
      "learning_rate": 1.2444444444444445e-05,
      "loss": 0.1309,
      "step": 845
    },
    {
      "epoch": 1.8888888888888888,
      "grad_norm": 0.6887403726577759,
      "learning_rate": 1.1333333333333334e-05,
      "loss": 0.1928,
      "step": 850
    },
    {
      "epoch": 1.9,
      "grad_norm": 0.558220624923706,
      "learning_rate": 1.0222222222222223e-05,
      "loss": 0.1905,
      "step": 855
    },
    {
      "epoch": 1.911111111111111,
      "grad_norm": 0.6445189714431763,
      "learning_rate": 9.111111111111112e-06,
      "loss": 0.1944,
      "step": 860
    },
    {
      "epoch": 1.9222222222222223,
      "grad_norm": 0.5120537281036377,
      "learning_rate": 8.000000000000001e-06,
      "loss": 0.1881,
      "step": 865
    },
    {
      "epoch": 1.9333333333333333,
      "grad_norm": 0.7108684778213501,
      "learning_rate": 6.888888888888889e-06,
      "loss": 0.1612,
      "step": 870
    },
    {
      "epoch": 1.9444444444444444,
      "grad_norm": 0.5592859983444214,
      "learning_rate": 5.777777777777778e-06,
      "loss": 0.157,
      "step": 875
    },
    {
      "epoch": 1.9555555555555557,
      "grad_norm": 0.5467049479484558,
      "learning_rate": 4.666666666666667e-06,
      "loss": 0.1638,
      "step": 880
    },
    {
      "epoch": 1.9666666666666668,
      "grad_norm": 0.5527376532554626,
      "learning_rate": 3.555555555555556e-06,
      "loss": 0.1529,
      "step": 885
    },
    {
      "epoch": 1.9777777777777779,
      "grad_norm": 0.6372949481010437,
      "learning_rate": 2.4444444444444447e-06,
      "loss": 0.176,
      "step": 890
    },
    {
      "epoch": 1.988888888888889,
      "grad_norm": 0.5724599957466125,
      "learning_rate": 1.3333333333333334e-06,
      "loss": 0.153,
      "step": 895
    },
    {
      "epoch": 2.0,
      "grad_norm": 0.7110374569892883,
      "learning_rate": 2.2222222222222224e-07,
      "loss": 0.1606,
      "step": 900
    }
  ],
  "logging_steps": 5,
  "max_steps": 900,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 2,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 1.8375079919616e+16,
  "train_batch_size": 2,
  "trial_name": null,
  "trial_params": null
}
