{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 1.0,
  "eval_steps": 500,
  "global_step": 450,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.011111111111111112,
      "grad_norm": 2.1057939529418945,
      "learning_rate": 0.00019911111111111111,
      "loss": 9.2581,
      "step": 5
    },
    {
      "epoch": 0.022222222222222223,
      "grad_norm": 39.238075256347656,
      "learning_rate": 0.00019800000000000002,
      "loss": 8.7243,
      "step": 10
    },
    {
      "epoch": 0.03333333333333333,
      "grad_norm": 121.95398712158203,
      "learning_rate": 0.0001968888888888889,
      "loss": 7.5214,
      "step": 15
    },
    {
      "epoch": 0.044444444444444446,
      "grad_norm": 10.359376907348633,
      "learning_rate": 0.0001957777777777778,
      "loss": 1.7064,
      "step": 20
    },
    {
      "epoch": 0.05555555555555555,
      "grad_norm": 61.395538330078125,
      "learning_rate": 0.0001946666666666667,
      "loss": 0.728,
      "step": 25
    },
    {
      "epoch": 0.06666666666666667,
      "grad_norm": 3.3489837646484375,
      "learning_rate": 0.00019355555555555557,
      "loss": 0.6549,
      "step": 30
    },
    {
      "epoch": 0.07777777777777778,
      "grad_norm": 2.307413339614868,
      "learning_rate": 0.00019244444444444444,
      "loss": 0.4489,
      "step": 35
    },
    {
      "epoch": 0.08888888888888889,
      "grad_norm": 0.7944432497024536,
      "learning_rate": 0.00019133333333333334,
      "loss": 0.4201,
      "step": 40
    },
    {
      "epoch": 0.1,
      "grad_norm": 0.5852090120315552,
      "learning_rate": 0.00019022222222222224,
      "loss": 0.3308,
      "step": 45
    },
    {
      "epoch": 0.1111111111111111,
      "grad_norm": 0.40772950649261475,
      "learning_rate": 0.00018911111111111112,
      "loss": 0.2863,
      "step": 50
    },
    {
      "epoch": 0.12222222222222222,
      "grad_norm": 1.2920374870300293,
      "learning_rate": 0.000188,
      "loss": 0.2771,
      "step": 55
    },
    {
      "epoch": 0.13333333333333333,
      "grad_norm": 0.5872239470481873,
      "learning_rate": 0.0001868888888888889,
      "loss": 0.2949,
      "step": 60
    },
    {
      "epoch": 0.14444444444444443,
      "grad_norm": 0.5821953415870667,
      "learning_rate": 0.0001857777777777778,
      "loss": 0.2414,
      "step": 65
    },
    {
      "epoch": 0.15555555555555556,
      "grad_norm": 0.9082880616188049,
      "learning_rate": 0.00018466666666666666,
      "loss": 0.2118,
      "step": 70
    },
    {
      "epoch": 0.16666666666666666,
      "grad_norm": 1.09840989112854,
      "learning_rate": 0.00018355555555555557,
      "loss": 0.2796,
      "step": 75
    },
    {
      "epoch": 0.17777777777777778,
      "grad_norm": 0.5194319486618042,
      "learning_rate": 0.00018244444444444447,
      "loss": 0.2483,
      "step": 80
    },
    {
      "epoch": 0.18888888888888888,
      "grad_norm": 0.48596271872520447,
      "learning_rate": 0.00018133333333333334,
      "loss": 0.2569,
      "step": 85
    },
    {
      "epoch": 0.2,
      "grad_norm": 0.42780640721321106,
      "learning_rate": 0.00018022222222222221,
      "loss": 0.2323,
      "step": 90
    },
    {
      "epoch": 0.2111111111111111,
      "grad_norm": 1.4466545581817627,
      "learning_rate": 0.00017911111111111112,
      "loss": 0.2601,
      "step": 95
    },
    {
      "epoch": 0.2222222222222222,
      "grad_norm": 0.6203381419181824,
      "learning_rate": 0.00017800000000000002,
      "loss": 0.2286,
      "step": 100
    },
    {
      "epoch": 0.23333333333333334,
      "grad_norm": 0.5583365559577942,
      "learning_rate": 0.0001768888888888889,
      "loss": 0.2288,
      "step": 105
    },
    {
      "epoch": 0.24444444444444444,
      "grad_norm": 0.6078658103942871,
      "learning_rate": 0.0001757777777777778,
      "loss": 0.2235,
      "step": 110
    },
    {
      "epoch": 0.25555555555555554,
      "grad_norm": 0.4208137094974518,
      "learning_rate": 0.00017466666666666667,
      "loss": 0.1914,
      "step": 115
    },
    {
      "epoch": 0.26666666666666666,
      "grad_norm": 0.5562767386436462,
      "learning_rate": 0.00017355555555555557,
      "loss": 0.2005,
      "step": 120
    },
    {
      "epoch": 0.2777777777777778,
      "grad_norm": 0.5547081232070923,
      "learning_rate": 0.00017244444444444444,
      "loss": 0.1979,
      "step": 125
    },
    {
      "epoch": 0.28888888888888886,
      "grad_norm": 0.5317286849021912,
      "learning_rate": 0.00017133333333333334,
      "loss": 0.1852,
      "step": 130
    },
    {
      "epoch": 0.3,
      "grad_norm": 0.5551788210868835,
      "learning_rate": 0.00017022222222222224,
      "loss": 0.2079,
      "step": 135
    },
    {
      "epoch": 0.3111111111111111,
      "grad_norm": 0.5516059398651123,
      "learning_rate": 0.00016911111111111112,
      "loss": 0.2116,
      "step": 140
    },
    {
      "epoch": 0.32222222222222224,
      "grad_norm": 0.56204754114151,
      "learning_rate": 0.000168,
      "loss": 0.1838,
      "step": 145
    },
    {
      "epoch": 0.3333333333333333,
      "grad_norm": 0.7104960083961487,
      "learning_rate": 0.0001668888888888889,
      "loss": 0.2359,
      "step": 150
    },
    {
      "epoch": 0.34444444444444444,
      "grad_norm": 0.5541868805885315,
      "learning_rate": 0.0001657777777777778,
      "loss": 0.2036,
      "step": 155
    },
    {
      "epoch": 0.35555555555555557,
      "grad_norm": 0.7651982307434082,
      "learning_rate": 0.00016466666666666667,
      "loss": 0.1916,
      "step": 160
    },
    {
      "epoch": 0.36666666666666664,
      "grad_norm": 0.4121015965938568,
      "learning_rate": 0.00016355555555555557,
      "loss": 0.1983,
      "step": 165
    },
    {
      "epoch": 0.37777777777777777,
      "grad_norm": 0.7105529308319092,
      "learning_rate": 0.00016244444444444444,
      "loss": 0.2244,
      "step": 170
    },
    {
      "epoch": 0.3888888888888889,
      "grad_norm": 0.48522520065307617,
      "learning_rate": 0.00016133333333333334,
      "loss": 0.2078,
      "step": 175
    },
    {
      "epoch": 0.4,
      "grad_norm": 0.5263397097587585,
      "learning_rate": 0.00016022222222222222,
      "loss": 0.2479,
      "step": 180
    },
    {
      "epoch": 0.4111111111111111,
      "grad_norm": 0.4443073570728302,
      "learning_rate": 0.00015911111111111112,
      "loss": 0.2195,
      "step": 185
    },
    {
      "epoch": 0.4222222222222222,
      "grad_norm": 0.9421856999397278,
      "learning_rate": 0.00015800000000000002,
      "loss": 0.1938,
      "step": 190
    },
    {
      "epoch": 0.43333333333333335,
      "grad_norm": 0.975707471370697,
      "learning_rate": 0.00015688888888888892,
      "loss": 0.233,
      "step": 195
    },
    {
      "epoch": 0.4444444444444444,
      "grad_norm": 0.48060333728790283,
      "learning_rate": 0.00015577777777777777,
      "loss": 0.1985,
      "step": 200
    },
    {
      "epoch": 0.45555555555555555,
      "grad_norm": 0.5299403667449951,
      "learning_rate": 0.00015466666666666667,
      "loss": 0.1993,
      "step": 205
    },
    {
      "epoch": 0.4666666666666667,
      "grad_norm": 0.5357173681259155,
      "learning_rate": 0.00015355555555555557,
      "loss": 0.2024,
      "step": 210
    },
    {
      "epoch": 0.4777777777777778,
      "grad_norm": 0.7542352080345154,
      "learning_rate": 0.00015244444444444447,
      "loss": 0.2013,
      "step": 215
    },
    {
      "epoch": 0.4888888888888889,
      "grad_norm": 0.7851024866104126,
      "learning_rate": 0.00015133333333333334,
      "loss": 0.2185,
      "step": 220
    },
    {
      "epoch": 0.5,
      "grad_norm": 0.4281010329723358,
      "learning_rate": 0.00015022222222222222,
      "loss": 0.1661,
      "step": 225
    },
    {
      "epoch": 0.5111111111111111,
      "grad_norm": 0.633613646030426,
      "learning_rate": 0.00014911111111111112,
      "loss": 0.1923,
      "step": 230
    },
    {
      "epoch": 0.5222222222222223,
      "grad_norm": 1.0158532857894897,
      "learning_rate": 0.000148,
      "loss": 0.1929,
      "step": 235
    },
    {
      "epoch": 0.5333333333333333,
      "grad_norm": 0.5630307793617249,
      "learning_rate": 0.0001468888888888889,
      "loss": 0.1749,
      "step": 240
    },
    {
      "epoch": 0.5444444444444444,
      "grad_norm": 0.5418225526809692,
      "learning_rate": 0.0001457777777777778,
      "loss": 0.2188,
      "step": 245
    },
    {
      "epoch": 0.5555555555555556,
      "grad_norm": 0.6214108467102051,
      "learning_rate": 0.0001446666666666667,
      "loss": 0.2175,
      "step": 250
    },
    {
      "epoch": 0.5666666666666667,
      "grad_norm": 0.6888212561607361,
      "learning_rate": 0.00014355555555555554,
      "loss": 0.2324,
      "step": 255
    },
    {
      "epoch": 0.5777777777777777,
      "grad_norm": 0.5525200366973877,
      "learning_rate": 0.00014244444444444444,
      "loss": 0.2181,
      "step": 260
    },
    {
      "epoch": 0.5888888888888889,
      "grad_norm": 0.6261931657791138,
      "learning_rate": 0.00014133333333333334,
      "loss": 0.191,
      "step": 265
    },
    {
      "epoch": 0.6,
      "grad_norm": 0.6424556970596313,
      "learning_rate": 0.00014022222222222225,
      "loss": 0.1943,
      "step": 270
    },
    {
      "epoch": 0.6111111111111112,
      "grad_norm": 0.9228836894035339,
      "learning_rate": 0.00013911111111111112,
      "loss": 0.242,
      "step": 275
    },
    {
      "epoch": 0.6222222222222222,
      "grad_norm": 0.4899066686630249,
      "learning_rate": 0.000138,
      "loss": 0.2159,
      "step": 280
    },
    {
      "epoch": 0.6333333333333333,
      "grad_norm": 0.6209553480148315,
      "learning_rate": 0.0001368888888888889,
      "loss": 0.2061,
      "step": 285
    },
    {
      "epoch": 0.6444444444444445,
      "grad_norm": 0.48287317156791687,
      "learning_rate": 0.00013577777777777777,
      "loss": 0.1878,
      "step": 290
    },
    {
      "epoch": 0.6555555555555556,
      "grad_norm": 0.5762816071510315,
      "learning_rate": 0.00013466666666666667,
      "loss": 0.2159,
      "step": 295
    },
    {
      "epoch": 0.6666666666666666,
      "grad_norm": 0.7060329914093018,
      "learning_rate": 0.00013355555555555557,
      "loss": 0.1963,
      "step": 300
    },
    {
      "epoch": 0.6777777777777778,
      "grad_norm": 0.5039269924163818,
      "learning_rate": 0.00013244444444444447,
      "loss": 0.1934,
      "step": 305
    },
    {
      "epoch": 0.6888888888888889,
      "grad_norm": 0.58073890209198,
      "learning_rate": 0.00013133333333333332,
      "loss": 0.2248,
      "step": 310
    },
    {
      "epoch": 0.7,
      "grad_norm": 0.6932163238525391,
      "learning_rate": 0.00013022222222222222,
      "loss": 0.1934,
      "step": 315
    },
    {
      "epoch": 0.7111111111111111,
      "grad_norm": 0.6120277047157288,
      "learning_rate": 0.00012911111111111112,
      "loss": 0.1545,
      "step": 320
    },
    {
      "epoch": 0.7222222222222222,
      "grad_norm": 0.5350877046585083,
      "learning_rate": 0.00012800000000000002,
      "loss": 0.1787,
      "step": 325
    },
    {
      "epoch": 0.7333333333333333,
      "grad_norm": 0.6189847588539124,
      "learning_rate": 0.0001268888888888889,
      "loss": 0.2091,
      "step": 330
    },
    {
      "epoch": 0.7444444444444445,
      "grad_norm": 0.7926344275474548,
      "learning_rate": 0.0001257777777777778,
      "loss": 0.2002,
      "step": 335
    },
    {
      "epoch": 0.7555555555555555,
      "grad_norm": 0.4579724073410034,
      "learning_rate": 0.00012466666666666667,
      "loss": 0.1867,
      "step": 340
    },
    {
      "epoch": 0.7666666666666667,
      "grad_norm": 0.9155359268188477,
      "learning_rate": 0.00012355555555555557,
      "loss": 0.1934,
      "step": 345
    },
    {
      "epoch": 0.7777777777777778,
      "grad_norm": 0.5608800649642944,
      "learning_rate": 0.00012244444444444445,
      "loss": 0.2067,
      "step": 350
    },
    {
      "epoch": 0.7888888888888889,
      "grad_norm": 0.5234827399253845,
      "learning_rate": 0.00012133333333333335,
      "loss": 0.1805,
      "step": 355
    },
    {
      "epoch": 0.8,
      "grad_norm": 0.4332217872142792,
      "learning_rate": 0.00012022222222222223,
      "loss": 0.1631,
      "step": 360
    },
    {
      "epoch": 0.8111111111111111,
      "grad_norm": 0.5168927311897278,
      "learning_rate": 0.00011911111111111111,
      "loss": 0.1895,
      "step": 365
    },
    {
      "epoch": 0.8222222222222222,
      "grad_norm": 0.46280208230018616,
      "learning_rate": 0.000118,
      "loss": 0.1673,
      "step": 370
    },
    {
      "epoch": 0.8333333333333334,
      "grad_norm": 0.5875239968299866,
      "learning_rate": 0.0001168888888888889,
      "loss": 0.1804,
      "step": 375
    },
    {
      "epoch": 0.8444444444444444,
      "grad_norm": 0.502194344997406,
      "learning_rate": 0.00011577777777777778,
      "loss": 0.1667,
      "step": 380
    },
    {
      "epoch": 0.8555555555555555,
      "grad_norm": 0.6969529390335083,
      "learning_rate": 0.00011466666666666667,
      "loss": 0.2035,
      "step": 385
    },
    {
      "epoch": 0.8666666666666667,
      "grad_norm": 0.5342328548431396,
      "learning_rate": 0.00011355555555555557,
      "loss": 0.1651,
      "step": 390
    },
    {
      "epoch": 0.8777777777777778,
      "grad_norm": 0.9851936101913452,
      "learning_rate": 0.00011244444444444445,
      "loss": 0.232,
      "step": 395
    },
    {
      "epoch": 0.8888888888888888,
      "grad_norm": 0.5667763948440552,
      "learning_rate": 0.00011133333333333333,
      "loss": 0.1842,
      "step": 400
    },
    {
      "epoch": 0.9,
      "grad_norm": 0.6381964087486267,
      "learning_rate": 0.00011022222222222222,
      "loss": 0.221,
      "step": 405
    },
    {
      "epoch": 0.9111111111111111,
      "grad_norm": 0.7211475968360901,
      "learning_rate": 0.00010911111111111112,
      "loss": 0.2211,
      "step": 410
    },
    {
      "epoch": 0.9222222222222223,
      "grad_norm": 0.5267907381057739,
      "learning_rate": 0.00010800000000000001,
      "loss": 0.1913,
      "step": 415
    },
    {
      "epoch": 0.9333333333333333,
      "grad_norm": 0.5003678202629089,
      "learning_rate": 0.00010688888888888891,
      "loss": 0.1897,
      "step": 420
    },
    {
      "epoch": 0.9444444444444444,
      "grad_norm": 0.6773400902748108,
      "learning_rate": 0.00010577777777777777,
      "loss": 0.1578,
      "step": 425
    },
    {
      "epoch": 0.9555555555555556,
      "grad_norm": 0.4976273477077484,
      "learning_rate": 0.00010466666666666667,
      "loss": 0.1808,
      "step": 430
    },
    {
      "epoch": 0.9666666666666667,
      "grad_norm": 0.666415810585022,
      "learning_rate": 0.00010355555555555556,
      "loss": 0.1994,
      "step": 435
    },
    {
      "epoch": 0.9777777777777777,
      "grad_norm": 0.46247631311416626,
      "learning_rate": 0.00010244444444444446,
      "loss": 0.196,
      "step": 440
    },
    {
      "epoch": 0.9888888888888889,
      "grad_norm": 0.6951578259468079,
      "learning_rate": 0.00010133333333333335,
      "loss": 0.1881,
      "step": 445
    },
    {
      "epoch": 1.0,
      "grad_norm": 0.9198401570320129,
      "learning_rate": 0.00010022222222222222,
      "loss": 0.2635,
      "step": 450
    }
  ],
  "logging_steps": 5,
  "max_steps": 900,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 2,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 9187539959808000.0,
  "train_batch_size": 2,
  "trial_name": null,
  "trial_params": null
}
